<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Javascript | Aaron Gustafson]]></title>
  <link href="http://aaron-gustafson.com//notebook/tags/javascript/atom.xml" rel="self"/>
  <link href="http://aaron-gustafson.com/"/>
  <updated>2014-09-15T08:16:02-04:00</updated>
  <id>http://aaron-gustafson.com/</id>
  <author>
    <name><![CDATA[Aaron Gustafson]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[A Fundamental Disconnect]]></title>
    <link href="http://aaron-gustafson.com/notebook/2014/a-fundamental-disconnect/"/>
    <updated>2014-09-13T15:01:58-04:00</updated>
    <id>http://aaron-gustafson.com/notebook/2014/a-fundamental-disconnect</id>
    <content type="html"><![CDATA[<p>Yesterday at <a href="2014.blendconf.com/">BlendConf</a>, <a href="http://www.hanselman.com/">Scott Hanselman</a> gave a fantastically-entertaining keynote entitled “JavaScript, The Cloud, and the rise of the New Virtual Machine.” In it, he chronicled all of the ways Web development and deployment has changed—for the better—over the years. He also boldly declared that JavaScript is now, effectively, a virtual machine in the browser.</p>

<p>This is a topic that has been weighing on my mind for quite some time now. I’ll start by saying that I’m a big fan of JavaScript. I write a lot of it and I find it incredibly useful, both as a programming language and as a way to improve the usability and accessibility of content on the Web. That said, I know it’s limitations. But I’ll get to that in a minute.</p>

<p>In the early days of the Web, “proper” software developers shied away from JavaScript. Many viewed it as a “toy” language (and felt similarly about HTML and CSS). It wasn’t as powerful as Java or Perl or C in their minds, so it wasn’t really worth learning. In the intervening years, however, JavaScript has changed a lot.</p>

<p>Most of these developers first began taking JavaScript seriously in the mid ’00s when Ajax became popular. And with the rise of JavaScript MVC frameworks and their ilk—Angular, Ember, etc.—many of these developers made their way onto the Web. I would argue that this, overall, is a good thing: We need more people working on the Web to make it better.</p>

<p>The one problem I’ve seen, however, is the fundamental disconnect many of these developers seem to have with the way deploying code on the Web works. In traditional software development, we have some say in the execution environment. On the Web, we don’t.</p>

<p>I’ll explain.</p>

<p>If we’re writing server-side software in Python or Rails or even PHP, one of two things is true:</p>

<ol>
<li>We control the server environment: operating system, language versions, packages, etc.; or</li>
<li>We don’t control the server environment, but we have knowledge of it and can author your program accordingly so it will execute as anticipated.</li>
</ol>


<p>In the more traditional installed software world, we can similarly control the environment by placing certain restrictions on what operating systems our code can run on and what the dependencies for its use may be in terms of hard drive space and RAM required. We provide that information up front and users can choose to use our software or use a competing product based on what will work for them.</p>

<p>On the Web, however, all bets are off. The Web is ubiquitous. The Web is messy. And, as much as we might like to control a user’s experience down to the very pixel, those of us who have been working on the Web for a while understand that it’s a fool’s errand and <a href="http://dowebsitesneedtolookexactlythesameineverybrowser.com/">have adjusted our expectations accordingly</a>. Unfortunately, this new crop of Web developers doesn’t seem to have gotten that memo.</p>

<p>We do not control the environment executing our JavaScript code, interpreting our HTML, or applying our CSS. Our users control the device (and, thereby, it’s processor speed, RAM, etc.). Our users choose the operating system. Our users pick the browser and which version they use. Our users can decide which add-ons they put in the browser. Our users can shrink or enlarge the fonts used to display our Web pages and apps. And the Internet providers that sit between us and our users, dictating the network speed, latency, and ultimately <a href="http://aaron-gustafson.com/notebook/2014/the-network-effect/">controlling how—and what part of—our content makes it to our users</a>.</p>

<p>All we can do is author an compelling, adaptive experience, cross our fingers, and hope for the best.</p>

<p>The fundamental problem with viewing JavaScript as the new VM is that it creates the illusion of control. Sure, if we are building an internal Web app, we might be able to dictate the OS/browser combination for all of our users and lock down their machines to prevent them from modifying those settings, but that’s not the reality on the open Web.</p>

<p>The fact is that we can’t absolutely rely on the availability of any specific technology when it comes to delivering a Web experience. Instead, we must look at <em>how</em> we construct that experience and make smarter decisions about how we use specific technologies in order to take advantage of their benefits while simultaneously understanding that they’re availability is not guaranteed. This is why progressive enhancement is such a useful philosophy.</p>

<p>The history of the Web is littered with JavaScript disaster stories. That doesn’t mean we shouldn’t use JavaScript or that it’s inherently bad. It simply means we need to be smarter about our approach to JavaScript and build robust experiences that allow users to do what they need to do even quickly an easily even if our carefully-crafted, incredibly well-designed JavaScript-driven interface won’t run.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[The Network Effect]]></title>
    <link href="http://aaron-gustafson.com/notebook/2014/the-network-effect/"/>
    <updated>2014-09-08T16:40:44-04:00</updated>
    <id>http://aaron-gustafson.com/notebook/2014/the-network-effect</id>
    <content type="html"><![CDATA[<p><cite>Ars Technica</cite> revealed today that <a href="http://arstechnica.com/tech-policy/2014/09/why-comcasts-javascript-ad-injections-threaten-security-net-neutrality/">Comcast is injecting self-promotional advertising into web pages delivered via it’s Wi-Fi hotspots</a>:</p>

<blockquote cite="http://arstechnica.com/tech-policy/2014/09/why-comcasts-javascript-ad-injections-threaten-security-net-neutrality/">
    <p>A Comcast spokesman told Ars the program began months ago. One facet of it is designed to alert consumers that they are connected to Comcast's Xfinity service. Other ads remind Web surfers to download Xfinity apps, Comcast spokesman Charlie Douglas told Ars in telephone interviews.</p>
</blockquote>


<p>I wish I could say this is surprising, but it’s not: Any service that routes your content has the opportunity to modify the response being returned. Comcast is exploiting that opportunity and injecting JavaScript that, in turn, injects the ads.</p>

<p>The fact that middlemen can manipulate server responses is one reason <a href="https://www.youtube.com/watch?v=cBhZ6S0PFCY">Google is pushing for all sites to be served under HTTPS</a>. With traffic running to and from your server in an encrypted fashion, <a href="http://en.wikipedia.org/wiki/Man-in-the-middle_attack">man-in-the-middle attacks</a>—which, if we’re honest, is what this amounts to—become much more difficult.</p>

<p>Assuming you can’t run under HTTPS for one reason or another, how do you harden your <a href="https://adactio.com/journal/6246">web thang</a> against 3rd party manipulation you can’t control? What if Comcast’s JavaScript interferes with your own? Remember when <a href="http://www.theguardian.com/technology/2014/jan/28/sky-broadband-blocks-jquery-web-critical-plugin">Sky blocked jQuery for all of their customers</a>? That was a bad couple of hours for most UK-based internet users.</p>

<p>Comcast’s move only serves to remind us—yet again—that we don’t control how our sites are delivered or what our users see. Or rather we do, but only up to a point. So rather than focus on some ideal experience we expect everyone to have, we must focus on building great experiences that work in a variety of contexts and situations.</p>

<p>We need to develop <a href="http://en.wikipedia.org/wiki/Demolition_derby#Vehicles">the 1964 Chrysler Imperial</a> of websites: Sites that soldier on even when they are getting pummeled from all sides. After all, browsers, plug-ins, users, networks, and, yes, even the very routers that deliver our connections all have a say in how (and what) content gets to our users.</p>

<p>I’ll leave you with this scary quote from the <cite>Ars</cite> piece:</p>

<blockquote cite="http://arstechnica.com/tech-policy/2014/09/why-comcasts-javascript-ad-injections-threaten-security-net-neutrality/">
    <p>Security expert Dan Kaminsky said in an e-mail that JavaScript injection has the potential to break “all sorts of stuff, in that you no longer know as a website developer precisely what code is running in browsers out there. You didn't send it, but your customers received it.”</p>
</blockquote>


<p>Hooray!</p>
]]></content>
  </entry>
  
</feed>
